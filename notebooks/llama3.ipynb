{"cells":[{"cell_type":"markdown","id":"620b192b-b363-4888-a482-5767229cc68e","metadata":{},"source":"Implementing LibraGrad From Scratch for Llama 3\n===============================================\n\n"},{"cell_type":"markdown","id":"cc16bb30-62cb-4f36-9aef-ca89c547dccc","metadata":{},"source":["You can open this notebook on [Google Colab](https://colab.research.google.com/github/NightMachinery/LibraGrad/blob/master/notebooks/llama3.ipynb).\n\n"]},{"cell_type":"markdown","id":"e9ac9b7a-0ffb-404b-8448-2f1272c593db","metadata":{},"source":["## Enable CUDA\n\n"]},{"cell_type":"markdown","id":"06fa4ae3-a476-4007-8ca2-10c4a4a9bf96","metadata":{},"source":["If the notebook has a CUDA-enabled runtime, we can use it:\n\n"]},{"cell_type":"code","execution_count":1,"id":"715f40f7-14e0-478a-b446-eeba735ba9b7","metadata":{},"outputs":[],"source":["import torch\n\nforce_cpu_p = False\nif not force_cpu_p and torch.cuda.is_available():\n    device = \"cuda\"\n    cuda_p = True\n\nelse:\n    device = \"cpu\"\n    cuda_p = False\n\ndevice"]},{"cell_type":"markdown","id":"fbc5f9f5-b43b-4cc7-abb0-1c339a7ed656","metadata":{},"source":["## Install Dependencies\n\n"]},{"cell_type":"code","execution_count":1,"id":"0a7d9e12-56ec-413d-aa5e-854b7b04897c","metadata":{},"outputs":[],"source":["! pip install -U pip\n\n! pip install -U pynight\n! pip install 'torch>=2.0.0'\n! pip install -U transformers datasets pillow 'matplotlib'\n! pip install -U 'numpy<2.0.0'"]},{"cell_type":"markdown","id":"6b04de2f-7c04-462f-9338-25bc79a09efe","metadata":{},"source":["You might need to restart the notebook in Colab for the latest version of the installed packages to be loaded.\n\n"]},{"cell_type":"markdown","id":"b9239a81-33d9-4010-be04-78570b5c7858","metadata":{},"source":["## Llama 3.2\n\n"]},{"cell_type":"code","execution_count":1,"id":"4ad4916e-8ab5-46be-b22d-527d00e2871f","metadata":{},"outputs":[],"source":["! hostname\n! date"]},{"cell_type":"code","execution_count":1,"id":"cf7f9b04-2c34-4157-b82c-88c560ce1f0f","metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_hf_name = \"unsloth/Llama-3.2-1B\"\n#: You can use any auto-regressive Llama 3 variant, e.g.:\n# model_hf_name = \"unsloth/Llama-3.2-3B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_hf_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_hf_name)\n##\nmodel.tokenizer = tokenizer\n\nmodel.to(device)\nNone"]},{"cell_type":"code","execution_count":1,"id":"7bda1394-bc03-48bb-ae4c-56adc289def0","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":1,"id":"ee97de28-6d99-499c-9a0a-5f9e052970b1","metadata":{},"outputs":[],"source":["print(model.__class__)"]},{"cell_type":"markdown","id":"b4f33922-806a-486a-90b4-0e62e5704bf0","metadata":{},"source":["### Helper Functions\n\n"]},{"cell_type":"markdown","id":"5dae161e-57c9-4c9a-8911-5fe112d9f8d7","metadata":{},"source":["Here we define various helper functions for predicting the next token and computing its attribution scores. Note that the code here is not part of LibraGrad&rsquo;s implementation; rather, here we are implementing the base attribution methods LibraGrad enhances.\n\nFirst let us import libraries we need:\n\n"]},{"cell_type":"code","execution_count":1,"id":"4b437e96-ac89-4a43-a1cf-05b64c05addd","metadata":{},"outputs":[],"source":["import os\nimport sys\nimport itertools\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom types import SimpleNamespace\nfrom typing import List, Optional, Union, Dict, Any, Tuple\nfrom transformers import PreTrainedTokenizer, PreTrainedModel\nfrom dataclasses import dataclass\nfrom torch import Tensor\nimport torch.utils.checkpoint\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib import colormaps\n\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom pynight.common_torch import (\n    module_mapper,\n    torch_shape_get,\n)\n\nfrom pynight.common_attr import (\n    normalize_map,\n)\n\nfrom pynight.common_icecream import ic"]},{"cell_type":"markdown","id":"654d35ce-86b7-4b70-9771-02cbf9bfe184","metadata":{},"source":["Now let us define some dataclasses for moving data around:\n\n"]},{"cell_type":"code","execution_count":1,"id":"ce741275-a084-426a-bb34-d1b1b5977f00","metadata":{},"outputs":[],"source":["@dataclass\nclass Attribution:\n    \"\"\"Stores attribution scores and completeness error.\"\"\"\n    scores: torch.Tensor\n    completeness_error: float\n\n@dataclass\nclass AttributionResults:\n    \"\"\"Stores both IxG and FullGrad+ attributions.\"\"\"\n    input_x_grad: Attribution\n    fullgrad_plus: Attribution\n\n@dataclass\nclass TopKComparisonItem:\n    \"\"\"Individual comparison item for top-k logits.\"\"\"\n    rank: int\n    original_idx: int\n    original_val: float\n    libra_idx: int\n    libra_val: float\n    same_index: bool\n    value_diff: float\n\n@dataclass\nclass LogitsComparison:\n    \"\"\"Results from comparing original and Libra model logits.\"\"\"\n    mse: float\n    cosine_similarity: float\n    max_absolute_diff: float\n    top_k_comparison: List[TopKComparisonItem]\n\n@dataclass\nclass ModelPrediction:\n    \"\"\"Model prediction results including token information and attributions.\"\"\"\n    next_token_str: str\n    next_token_id: int\n    logits: torch.Tensor\n    probs: torch.Tensor\n    input_ids: List[int]\n    token_texts: Optional[List[str]] = None\n    attributions: Optional[AttributionResults] = None"]},{"cell_type":"markdown","id":"16b9ba2b-e14b-4d78-8972-23edbc8e5c28","metadata":{},"source":["Now we implement the next token prediction and its Input-X-Grad (IxG) and FullGrad+ attributions. Llama3 has no biases, so implementing FullGrad+ on it is straightforward. Also, IxG is equivalent to FullGrad on this model, and thus Libra IxG should also have zero completeness error.\n\n"]},{"cell_type":"code","execution_count":1,"id":"df1c0daa-f5f2-457d-a1f9-57dd672913f5","metadata":{},"outputs":[],"source":["def forward_hook_collector(\n    module: torch.nn.Module,\n    inputs: Tuple[torch.Tensor, ...],\n    outputs: Any,\n    layer_inputs: List[torch.Tensor],\n) -> None:\n    \"\"\"Forward hook to collect input tensors from model layers.\"\"\"\n    tensor_input = inputs[0] if isinstance(inputs, tuple) else inputs\n    \n    tensor_input.requires_grad_(True)\n    tensor_input.retain_grad()\n    \n    layer_inputs.append(tensor_input)\n\ndef get_model_outputs(\n    model: PreTrainedModel,\n    input_ids: torch.Tensor,\n    embeddings: Optional[torch.Tensor] = None,\n) -> Tuple[torch.Tensor, torch.Tensor, int]:\n    \"\"\"\n    Helper function to compute model outputs and next token prediction.\n    \"\"\"\n    if getattr(model.config, 'pretraining_tp', 0) > 1:\n        raise ValueError(\"Tensor parallel models are not supported\")\n\n    device = next(model.parameters()).device\n    input_ids = input_ids.to(device)\n\n    with torch.no_grad():\n        attention_mask = torch.ones_like(input_ids, device=device)\n\n    model_inputs = {\n        'attention_mask': attention_mask.unsqueeze(0),\n        'return_dict': True,\n        'output_hidden_states': True,  # Needed for FullGrad+\n        'num_logits_to_keep': 1,       # Only compute logits for the last token\n    }\n\n    if embeddings is not None:\n        model_inputs['inputs_embeds'] = embeddings.unsqueeze(0).to(device)\n    else:\n        model_inputs['input_ids'] = input_ids.unsqueeze(0)\n\n    outputs = model(**model_inputs)\n\n    # Get logits for the last position only\n    logits = outputs.logits[:, -1, :]  # Shape: (1, vocab_size)\n\n    with torch.no_grad():\n        probs = F.softmax(logits.detach(), dim=-1)\n        next_token_id = torch.argmax(logits.detach(), dim=-1).item()\n\n    return logits, probs, next_token_id\n\ndef predict_next_token(\n    text: str,\n    *,\n    model: PreTrainedModel,\n    tokenizer: Union[str, PreTrainedTokenizer] = \"from_model\",\n    attribute_p: bool = False,\n) -> ModelPrediction:\n    \"\"\"\n    Predicts the next token and optionally calculates attributions.\n    \"\"\"\n    # Handle tokenizer\n    if tokenizer == \"from_model\":\n        tokenizer = model.tokenizer\n\n    device = next(model.parameters()).device\n\n    # Tokenize input text - no need for gradients here\n    with torch.no_grad():\n        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids[0].to(device)\n        token_texts = [tokenizer.decode([id]) for id in input_ids.cpu().tolist()]\n\n    if not attribute_p:\n        # Simple forward pass without attribution\n        with torch.no_grad():\n            logits, probs, next_token_id = get_model_outputs(model, input_ids)\n            logits = logits.detach()\n            probs = probs.detach()\n\n        return ModelPrediction(\n            next_token_str=tokenizer.decode([next_token_id]),\n            next_token_id=next_token_id,\n            logits=logits.squeeze(0),\n            probs=probs.squeeze(0),\n            input_ids=input_ids.cpu().tolist(),\n            token_texts=token_texts,\n        )\n\n    # Attribution computation\n    model.zero_grad()\n\n    embeddings = model.model.embed_tokens(input_ids)\n    embeddings.requires_grad_(True)\n    embeddings.retain_grad()\n\n    layer_inputs: List[torch.Tensor] = []\n    hooks = []\n\n    for layer in model.model.layers:\n        hook = layer.register_forward_hook(\n            lambda mod, inp, out, li=layer_inputs: forward_hook_collector(mod, inp, out, li)\n        )\n        hooks.append(hook)\n\n    logits, probs, next_token_id = get_model_outputs(model, input_ids, embeddings)\n    target_logit = logits[0, next_token_id]\n\n    # Remove hooks before backward pass\n    for hook in hooks:\n        hook.remove()\n\n    target_logit.backward()\n    # ic(torch_shape_get(embeddings.grad))\n\n    # Calculate attributions - no need for gradients here\n    with torch.no_grad():\n        # Calculate Input x Gradient attributions\n        ixg_attr = (embeddings * embeddings.grad).sum(dim=-1).detach()\n        ixg_completeness_error = abs(\n            ixg_attr.sum().item() - target_logit.detach().item()\n        )\n\n        # Calculate FullGrad+ attributions by averaging IxG across all layer inputs\n        fullgrad_attrs = []\n        for layer_input in layer_inputs:\n            if layer_input.grad is not None:\n                attr = (layer_input * layer_input.grad).sum(dim=-1).detach()\n                # ic(attr.shape)\n                #: attr: [batch=1, tokens]\n                \n                if len(attr.shape) > 1:\n                    attr = attr[0]\n                fullgrad_attrs.append(attr)\n\n        if fullgrad_attrs:\n            fullgrad_plus = torch.stack(fullgrad_attrs).mean(dim=0).detach()\n        else:\n            fullgrad_plus = torch.zeros_like(ixg_attr)\n\n        # Calculate completeness error for FullGrad+\n        fullgrad_completeness_error = abs(\n            fullgrad_plus.sum().item() - target_logit.detach().item()\n        )\n\n        # Move tensors to CPU and detach before storing\n        attributions = AttributionResults(\n            input_x_grad=Attribution(\n                scores=ixg_attr.cpu().detach(),\n                completeness_error=ixg_completeness_error\n            ),\n            fullgrad_plus=Attribution(\n                scores=fullgrad_plus.cpu().detach(),\n                completeness_error=fullgrad_completeness_error\n            )\n        )\n\n    # Detach results before returning\n    return ModelPrediction(\n        next_token_str=tokenizer.decode([next_token_id]),\n        next_token_id=next_token_id,\n        logits=logits.detach().squeeze(0),\n        probs=probs.detach().squeeze(0),\n        input_ids=input_ids.cpu().tolist(),\n        token_texts=token_texts,\n        attributions=attributions\n    )"]},{"cell_type":"markdown","id":"7e16cbce-00e1-408b-bea0-d108842f226e","metadata":{},"source":["We now define some helper functions for comparing the outputs of the model, which we&rsquo;ll later use to verify that LibraGrad has not changed the forward pass:\n\n"]},{"cell_type":"code","execution_count":1,"id":"19900f74-f59a-4519-90bd-cf3cb498b333","metadata":{},"outputs":[],"source":["def compare_logits(\n    original_logits: torch.Tensor,\n    libra_logits: torch.Tensor,\n    top_k: int = 3,\n) -> LogitsComparison:\n    \"\"\"\n    Compares logits between original and Libra models.\n    \"\"\"\n    with torch.no_grad():\n        # Basic similarity metrics\n        mse = F.mse_loss(original_logits.detach(), libra_logits.detach()).item()\n        cos_sim = F.cosine_similarity(\n            original_logits.detach().unsqueeze(0),\n            libra_logits.detach().unsqueeze(0)\n        ).item()\n        max_abs_diff = torch.max(torch.abs(\n            original_logits.detach() - libra_logits.detach()\n        )).item()\n\n        # Get top K indices and values\n        original_top_k = torch.topk(original_logits.detach(), top_k)\n        libra_top_k = torch.topk(libra_logits.detach(), top_k)\n\n        # Create detailed comparison items\n        top_k_comparison = []\n        for i in range(top_k):\n            comparison_item = TopKComparisonItem(\n                rank=i + 1,\n                original_idx=original_top_k.indices[i].item(),\n                original_val=original_top_k.values[i].item(),\n                libra_idx=libra_top_k.indices[i].item(),\n                libra_val=libra_top_k.values[i].item(),\n                same_index=original_top_k.indices[i].item() == libra_top_k.indices[i].item(),\n                value_diff=abs(original_top_k.values[i].item() - libra_top_k.values[i].item())\n            )\n            top_k_comparison.append(comparison_item)\n\n    return LogitsComparison(\n        mse=mse,\n        cosine_similarity=cos_sim,\n        max_absolute_diff=max_abs_diff,\n        top_k_comparison=top_k_comparison\n    )\n\ndef print_logits_comparison(comparison_results: LogitsComparison) -> None:\n    \"\"\"Prints the logits comparison results in a readable format.\"\"\"\n    print(\"Overall Metrics:\")\n    print(f\"MSE: {comparison_results.mse:.6f}\")\n    print(f\"Cosine Similarity: {comparison_results.cosine_similarity:.6f}\")\n    print(f\"Max Absolute Difference: {comparison_results.max_absolute_diff:.6f}\")\n\n    print(\"\\nTop-K Comparison:\")\n    print(\"Rank  Original(idx,val)     Libra(idx,val)        Same?  Diff\")\n    print(\"-\" * 70)\n\n    for comp in comparison_results.top_k_comparison:\n        same_marker = \"✓\" if comp.same_index else \"✗\"\n        print(\n            f\"{comp.rank:2d}    ({comp.original_idx:5d}, {comp.original_val:8.3f})    \"\n            f\"({comp.libra_idx:5d}, {comp.libra_val:8.3f})    \"\n            f\"{same_marker}     {comp.value_diff:.6f}\"\n        )"]},{"cell_type":"markdown","id":"2c59cceb-68fc-43df-bd4a-2e480abe50e5","metadata":{},"source":["#### Visualization\n\n"]},{"cell_type":"markdown","id":"0cdccf0a-7517-4dec-ba47-7180e5635946","metadata":{},"source":["Here we define some utilities to visualize the attribution scores.\n\n"]},{"cell_type":"markdown","id":"0fe49f4a-dc51-447b-bf88-2a31d49fa640","metadata":{},"source":["##### Plotter\n\n"]},{"cell_type":"code","execution_count":1,"id":"b440dfd1-5b57-4886-a79e-c409fd0718ea","metadata":{},"outputs":[],"source":["def plot_attributions_PIL_v2(\n    token_texts,\n    attributions,\n    title,\n    height=70,  # Height in pixels\n    pos_cmap=\"Blues\",\n    neg_cmap=\"Oranges\",\n    show_p=True,\n    dpi=300,\n    font_size=42,\n    white_foreground_threshold=0.6,\n):\n    \"\"\"\n    Plots the attributions by coloring token backgrounds using PIL.\n    Positive scores: white to blue (or custom colormap).\n    Negative scores: white to orange (or custom colormap).\n    Args:\n        token_texts (List[str]): List of token strings.\n        attributions (torch.Tensor): Tensor of normalized attribution scores (expected in [-1, 1]).\n        title (str): Plot title.\n        height (int): Height of the plot in pixels. Default is 70.\n        pos_cmap (str): Colormap name for positive scores. Default is \"Blues\".\n        neg_cmap (str): Colormap name for negative scores. Default is \"Oranges\".\n        show_p (bool): If True, displays the image. Default is True.\n        dpi (int): Dots per inch. Default is 300.\n        font_size (int): Font size in points. Default is 12.\n        white_foreground_threshold (float): Threshold for absolute attribution score above which text color becomes white. Default is 0.6.\n    Returns:\n        PIL.Image: The generated image\n    \"\"\"\n    # Convert attributions to numpy if it's a torch tensor\n    if hasattr(attributions, \"cpu\"):\n        attributions = attributions.cpu().numpy()\n\n    # Get colormaps\n    pos_colormap = colormaps[pos_cmap]\n    neg_colormap = colormaps[neg_cmap]\n\n    # Try to load a TrueType font with specified size\n    try:\n        # Extended font paths including Colab paths\n        font_paths = [\n            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",  # Linux\n            \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\",  # Colab\n            \"/Library/Fonts/Arial.ttf\",  # macOS\n            \"C:\\\\Windows\\\\Fonts\\\\arial.ttf\",  # Windows\n        ]\n\n        font = None\n        for path in font_paths:\n            if os.path.exists(path):\n                try:\n                    font = ImageFont.truetype(path, font_size)\n                    break\n                except Exception:\n                    continue\n\n        if font is None:\n            # If no system fonts work, try downloading and using a Google Font\n            try:\n                import urllib.request\n                import tempfile\n\n                # Download Roboto font from Google Fonts\n                font_url = \"https://github.com/google/fonts/raw/main/apache/roboto/Roboto-Regular.ttf\"\n                temp_font_path = tempfile.mktemp(suffix=\".ttf\")\n                urllib.request.urlretrieve(font_url, temp_font_path)\n\n                font = ImageFont.truetype(temp_font_path, font_size)\n                print(\"Using downloaded Roboto font.\")\n            except Exception as e:\n                # Fallback to default font if download fails\n                font = ImageFont.load_default()\n                print(\n                    f\"Warning: Could not download font ({str(e)}). Using default font.\"\n                )\n    except Exception as e:\n        font = ImageFont.load_default()\n        print(f\"Warning: Font loading error ({str(e)}). Using default font.\")\n\n    # Constants for layout\n    padding_horizontal = font_size  # Scale padding with font size\n    padding_vertical = font_size // 2\n    title_height = int(font_size * 2.5)  # More space for title\n    text_height = height - 2 * padding_vertical\n    total_height = height + title_height + padding_vertical\n\n    # Create temporary drawing surface for measurements\n    temp_img = Image.new(\"RGB\", (1, 1), \"white\")\n    draw = ImageDraw.Draw(temp_img)\n\n    # Calculate exact token widths and total width\n    token_sizes = [draw.textbbox((0, 0), token, font=font) for token in token_texts]\n    token_widths = [bbox[2] - bbox[0] + padding_horizontal * 2 for bbox in token_sizes]\n    total_width = sum(token_widths)\n\n    # Measure title width\n    title_bbox = draw.textbbox((0, 0), title, font=font)\n    title_width = title_bbox[2] - title_bbox[0]\n\n    # Ensure total width is at least as wide as title\n    total_width = max(total_width, title_width + padding_horizontal * 2)\n\n    # Create final image with white background\n    img = Image.new(\"RGB\", (total_width, total_height), \"white\")\n\n    # Set DPI in the image metadata\n    img.info[\"dpi\"] = (dpi, dpi)\n\n    draw = ImageDraw.Draw(img)\n\n    # Helper function to get color from matplotlib colormap\n    def get_color(score, is_positive=True):\n        if is_positive:\n            rgb = pos_colormap(score)\n        else:\n            rgb = neg_colormap(score)\n        return tuple(int(x * 255) for x in rgb[:3])\n\n    # Draw title\n    title_x = (total_width - title_width) // 2\n    title_y = padding_vertical\n    draw.text((title_x, title_y), title, fill=\"black\", font=font)\n\n    # Draw tokens with backgrounds\n    current_x = 0\n    for token, width, score in zip(token_texts, token_widths, attributions):\n        # Calculate background color\n        if score >= 0:\n            color = get_color(score, is_positive=True)\n        else:\n            color = get_color(-score, is_positive=False)\n\n        # Draw background rectangle\n        draw.rectangle(\n            [(current_x, title_height), (current_x + width, total_height)], fill=color\n        )\n\n        # Calculate exact text position for centering\n        text_bbox = draw.textbbox((0, 0), token, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        text_x = current_x + (width - text_width) // 2\n        text_y = title_height + (height - text_height) // 2\n\n        # Determine text color based on attribution score\n        text_color = \"white\" if abs(score) > white_foreground_threshold else \"black\"\n\n        # Draw token text\n        draw.text((text_x, text_y), token, fill=text_color, font=font)\n\n        current_x += width\n\n    # Display the image if show_p is True\n    if show_p:\n        try:\n            from IPython.display import display\n\n            display(img)\n\n        except ImportError:\n            # Use regular show method\n            img.show()\n\n    return img\n\n\ndef save_with_dpi(img, filename, dpi=300):\n    \"\"\"\n    Helper function to save an image with specific DPI.\n    Args:\n        img (PIL.Image): The image to save\n        filename (str): Output filename\n        dpi (int): Dots per inch. Default is 300.\n    \"\"\"\n    # Save with DPI information\n    img.save(filename, dpi=(dpi, dpi))"]},{"cell_type":"code","execution_count":1,"id":"f647c031-29bf-48d5-aa7c-6399c24b9ff7","metadata":{},"outputs":[],"source":["def analyze_test_cases(\n    test_cases,\n    model,\n    plot_ixg=True,\n    plot_fullgrad=True,\n    plot_fn=None,\n):\n    \"\"\"\n    Analyzes test cases using the model and plots attributions.\n\n    Args:\n        model: The model to use for predictions\n        plot_ixg: Whether to plot Input x Gradient attributions\n        plot_fullgrad: Whether to plot FullGrad+ attributions\n    \"\"\"\n    global global_libra_mode_p\n\n    if plot_fn is None:\n        plot_fn = plot_attributions_PIL_v2\n\n    for text, global_libra_mode_p in itertools.product(test_cases, [False, True]):\n        # print(f\"\\nAnalyzing: {text!r}\")\n\n        if global_libra_mode_p:\n            method_prefix = \"Libra \"\n        else:\n            method_prefix = \"\"\n\n        #: Get predictions and attributions\n        result = predict_next_token(\n            text,\n            model=model,\n            attribute_p=True,\n        )\n\n        # print(f\"Predicted next token: {result.next_token_str!r}\")\n\n        #: Print completeness errors for both methods\n        print(\n            f\"{method_prefix}Input-X-Grad Completeness Error: {result.attributions.input_x_grad.completeness_error:.6f}\"\n        )\n        print(\n            f\"{method_prefix}FullGrad+ Completeness Error: {result.attributions.fullgrad_plus.completeness_error:.6f}\"\n        )\n\n        if plot_ixg:\n            normalized_ixg = normalize_map(\n                result.attributions.input_x_grad.scores,\n                normalize=[\"scale_by_max_signed_attr\"],\n                outlier_quantile=0.01,\n            ).attributions_normalized.clamp(max=1, min=-1)\n\n            title = f\"{method_prefix}Input-X-Grad Attributions for Next Token Prediction: {result.next_token_str!r}\"\n            #: The =!r= in Python string formatting is a conversion flag that calls the repr() function on the value before formatting it. This means the value will be displayed with quotes and escape characters as needed, showing its \"official\" string representation.\n\n            plot_fn(\n                result.token_texts,\n                normalized_ixg,\n                title=title,\n            )\n\n        if plot_fullgrad:\n            normalized_fg = normalize_map(\n                result.attributions.fullgrad_plus.scores,\n                normalize=[\"scale_by_max_signed_attr\"],\n                outlier_quantile=0.01,\n            ).attributions_normalized.clamp(max=1, min=-1)\n\n            title = f\"{method_prefix}FullGrad+ Attributions for Next Token Prediction: {result.next_token_str!r}\"\n\n            plot_fn(\n                result.token_texts,\n                normalized_fg,\n                title=title,\n            )"]},{"cell_type":"markdown","id":"3fe38190-4416-4842-8207-8244b1d185c4","metadata":{},"source":["### Test Samples\n\n"]},{"cell_type":"markdown","id":"aaed51da-8456-421c-8d62-8c021ae54b32","metadata":{},"source":["Here we define some example sentences for the next token prediction task we are testing the models on.\n\n"]},{"cell_type":"code","execution_count":1,"id":"17388f4e-7712-4d99-b648-64736b8ee866","metadata":{},"outputs":[],"source":["test_cases = [\n    # Basic knowledge completion\n    \"The chemical symbol for gold is\",  # Expected: Au\n    \"The largest planet in our solar system is\",  # Expected: Jupiter\n    \"The speed of light in meters per second is approximately \",  # Expected: 299792458\n    \n    # Mathematical reasoning\n    \"If x + 5 = 12, then x equals\",  # Expected: 7\n    \"The square root of 144 is\",  # Expected: 12\n    \"The next number in the sequence: 2, 4, 8, 16,\",  # Expected: 32\n    \n    # Language patterns and idioms\n    \"Birds of a feather flock\",  # Expected: together\n    \"When in Rome, do as the Romans\",  # Expected: do\n    \"The early bird catches the\",  # Expected: worm\n    \n    # Context-dependent completion\n    \"In chess, the piece that moves only diagonally is the\",  # Expected: bishop\n    \"In baseball, three strikes and you're\",  # Expected: out\n    \"In music, forte means to play\",  # Expected: loud/loudly\n    \n    # Multi-token reasoning\n    \"If today is Monday, the day after tomorrow will be\",  # Expected: Wednesday\n    \"The opposite of artificial intelligence is natural\",  # Expected: stupidity\n    \"Water freezes at 0°C and boils at\",  # Expected: 100°C\n    \n    # Common knowledge with a twist\n    \"Sharks are fish, but dolphins are\",  # Expected: mammals\n    \"The Earth is round, but it's actually shaped like a\",  # Expected: spheroid/ellipsoid\n    \"A tomato is technically a fruit, but in cooking it's used as a\",  # Expected: vegetable\n    \n    # Pattern completion with multiple valid answers\n    \"Red, Orange, Yellow, Green, Blue,\",  # Expected: Indigo/Purple/Violet\n    \"Spring, Summer, Fall,\",  # Expected: Winter\n    \n    # Technical completions\n    \"In Python, a list is mutable but a tuple is\",  # Expected: immutable\n    \"HTTP status code 404 means page not\",  # Expected: found\n    \"RAM stands for Random Access\",  # Expected: Memory\n    \n    # Logical reasoning\n    \"If all A are B, and all B are C, then all A are\",  # Expected: C\n    \"If it's raining, the ground is wet. The ground is dry, therefore it's\",  # Expected: not raining\n    \"Every action has an equal and opposite\",  # Expected: reaction\n    \n    # Cultural references\n    \"Luke, I am your\",  # Expected: father\n    \"To be, or not to\",  # Expected: be\n    \"Houston, we have a\",  # Expected: problem\n    \n    # Time-based patterns\n    \"January, February, March,\",  # Expected: April\n    \"Monday, Tuesday, Wednesday,\",  # Expected: Thursday\n    \"Dawn, morning, noon, afternoon,\",  # Expected: evening/dusk\n    \n    # Numerical patterns with complexity\n    \"2, 3, 5, 7, 11,\",  # Expected: 13 (prime numbers)\n    \"1, 1, 2, 3, 5, 8,\",  # Expected: 13 (Fibonacci)\n    \"1, 4, 9, 16, 25,\",  # Expected: 36 (square numbers)\n    \n    # Geographic knowledge\n    \"The Great Wall is in\",  # Expected: China\n    \"The Amazon Rainforest is primarily in\",  # Expected: Brazil\n    \"Mount Everest is located in\",  # Expected: Nepal/Tibet\n    \n    # Scientific concepts\n    \"E equals mc\",  # Expected: squared\n    \"The human body is made up of approximately 60% \",  # Expected: water\n    \"The closest star to Earth is the\",  # Expected: Sun\n]\n\n# Test cases focusing on ambiguity and context\nambiguous_test_cases = [\n    # Same prompt, different contexts\n    \"The word 'bank' can refer to a financial institution or the edge of a\",  # Expected: river\n    \"A mouse can be a rodent or a computer\",  # Expected: device/peripheral\n    \"Java can be a programming language or a type of\",  # Expected: coffee\n    \n    # Contextual completion\n    \"In biology, a cell is basic unit of life, but in prison it's a\",  # Expected: room\n    \"The word 'bright' can describe intelligence or\",  # Expected: light\n    \"The term 'hard drive' in computing refers to storage, but 'hard' alone means\",  # Expected: difficult\n]\n\n# Test cases for numerical reasoning\nnumerical_test_cases = [\n    # Mathematical patterns\n    \"1/4 of 100 is\",  # Expected: 25\n    \"A dozen dozens is\",  # Expected: 144\n    \"The next power of 2: 2,4,8,16,32,\",  # Expected: 64\n    \n    # Unit conversions\n    \"There are 1000 meters in a\",  # Expected: kilometer\n    \"There are 100 centimeters in a\",  # Expected: meter\n    \"There are 60 seconds in a\",  # Expected: minute\n]\n\n# Test cases requiring reference to previous context\nreference_test_cases = [\n    # Simple name references\n    \"The student's name is Alice Lovely. Her name starts with the letter\",  # Expected: A\n    \"My friend Bob Wilson lives in Paris. Wilson's first name starts with the character '\",  # Expected: B\n    \"Dr. James Smith teaches biology. Dr. Smith's given name is\",\n    \n    # Multiple references to choose from\n    \"Mohammad met Mary at a cafe. The person whose name is shorter is\",  # Expected: Mohammad\n    \"The cities Paris and Rome are beautiful. The city with more letters is\",  # Expected: Paris\n    \n    # Numerical references\n    \"The temperature was 23 degrees yesterday and 25 today. The difference is \",  # Expected: 2\n    \"Room A has 15 chairs and Room B has 20 chairs. The smaller number is\",  # Expected: 15\n    \n    # Complex references\n    \"The red car costs $20000 and the blue car costs $25000. The cheaper car is the\",  # Expected: red\n    \"In the story, Sarah is 12 and her brother Tom is 8. The older sibling's name is\",  # Expected: Sarah\n    \n    # Attribute references\n    \"The book has a blue cover and golden pages. The color of the cover is\",  # Expected: blue\n    \"The painting shows a sunset over mountains. The scene takes place during\",  # Expected: sunset\n    \n    # Multi-token references\n    \"James Bond ordered a martini, shaken not stirred. His drink preference was\",  # Expected: martini\n    \"The recipe calls for olive oil and balsamic vinegar. The first ingredient is\",  # Expected: olive oil\n    \n    # Time and sequence references\n    \"Monday comes before Tuesday. The earlier day is\",  # Expected: Monday\n    \"Spring arrives after winter. The colder season is\",  # Expected: winter\n    \n    # Location references\n    \"The keys are either in the kitchen or the bedroom. The room that starts with 'k' is\",  # Expected: kitchen\n    \"Between New York and Paris, the American city is\",  # Expected: New York\n    \n    # Property references\n    \"Diamonds are hard and clouds are soft. The harder object is the\",  # Expected: Diamonds\n    \"A cheetah runs fast while a turtle moves slowly. The faster animal is\",  # Expected: cheetah\n    \n    # References with distractors\n    \"Although Maria likes blue, Juan's favorite color is red. Maria prefers the color\",  # Expected: blue\n    \"While the square has 4 sides, the triangle drawn in red has 3. The shape with more sides is called a\",  # Expected: square\n]\n\nall_test_cases = [\n    *reference_test_cases,\n    *test_cases,\n    *numerical_test_cases,\n    *ambiguous_test_cases,\n]"]},{"cell_type":"markdown","id":"d96d3389-7039-40fe-ab1e-f414a120e4bc","metadata":{},"source":["### Standard Gradients\n\n"]},{"cell_type":"markdown","id":"9c8abbc1-4f3c-4249-9897-aab176a7461d","metadata":{},"source":["#### Store Forward Pass Outputs of Standard Model\n\n"]},{"cell_type":"markdown","id":"bc5c3690-b13b-49ad-83b6-b4a6483f432d","metadata":{},"source":["Prior to implementing modifications, we&rsquo;ll execute a forward pass with the original model to obtain baseline token predictions, enabling subsequent verification of forward pass consistency.\n\n"]},{"cell_type":"code","execution_count":1,"id":"e7b459bb-a4b0-4507-89df-3f92b49d8283","metadata":{},"outputs":[],"source":["text = \"Once upon a\"\noriginal_pred = predict_next_token(text, model=model)"]},{"cell_type":"markdown","id":"f74ad85e-7c42-4949-b891-a41a212ba80d","metadata":{},"source":["### Libra Layers\n\n"]},{"cell_type":"markdown","id":"ff8e6831-3321-45a0-83b8-91764bcdebb2","metadata":{},"source":["We now implement LibraGrad for the Llama 3 model architecture.\n\n"]},{"cell_type":"code","execution_count":1,"id":"4e88805d-472b-455f-9ccc-2e48c4200110","metadata":{},"outputs":[],"source":["global_libra_mode_p = True\nglobal_libra_verbose_p = False\n\ndef print_libra(*args, **kwargs):\n    if global_libra_verbose_p:\n        print(*args, **kwargs)"]},{"cell_type":"code","execution_count":1,"id":"89f3169b-af2e-4cdb-a8ca-2571ba9c9bcd","metadata":{},"outputs":[],"source":["def swap_backward(forward, backward):\n    return forward.detach() + (backward - backward.detach())"]},{"cell_type":"markdown","id":"65da05c1-5e5b-44ae-91ce-e2c6a3a6958a","metadata":{},"source":["#### SiLU\n\n"]},{"cell_type":"code","execution_count":1,"id":"755894eb-622c-4a52-8271-be33863ac317","metadata":{},"outputs":[],"source":["def libra_silu(\n    x,\n    *,\n    libra_p=\"from_global\",\n    inplace_p=False,\n):\n    \"\"\"Swish (or Silu) activation function.\n\n    It is defined as: `swish(x) = x * sigmoid(x)`.\n\n    The Swish (or Silu) activation function is a smooth,\n    non-monotonic function that is unbounded above and\n    bounded below.\n\n    Args:\n        x: Input tensor.\n\n    Reference:\n\n    - [Ramachandran et al., 2017](https://arxiv.org/abs/1710.05941)\n    \"\"\"\n    # print_libra(f\"libra_silu entered: libra_p={libra_p}\")\n\n    if libra_p == \"from_global\":\n        libra_p = global_libra_mode_p\n\n    sigmoid_x = F.sigmoid(x)\n\n    if libra_p:\n        print_libra(\"Libra SiLU\")\n\n        sigmoid_x = sigmoid_x.detach()\n\n    if inplace_p:\n        return x.mul_(sigmoid_x)\n    else:\n        return x * sigmoid_x\n\n\nclass LibraSiLU(nn.SiLU):\n    def __init__(self, inplace: bool = False):\n        super().__init__(inplace)\n        self.prefix = f\"unset.{self.__class__.__name__}\"\n\n    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n        self.prefix = prefix\n        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n\n    def forward(\n        self,\n        input: torch.Tensor,\n        *,\n        libra_p=\"from_global\",\n    ) -> torch.Tensor:\n        return libra_silu(\n            input,\n            libra_p=libra_p,\n            inplace_p=self.inplace,\n        )"]},{"cell_type":"markdown","id":"c6fb9c85-bf0a-4a71-95d4-db562e1abfa8","metadata":{},"source":["#### LlamaMLP\n\n"]},{"cell_type":"code","execution_count":1,"id":"9594f211-97aa-4434-99c8-1b4fa1d01a03","metadata":{},"outputs":[],"source":["class LibraLlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(\n            self.hidden_size, self.intermediate_size, bias=config.mlp_bias\n        )\n        self.up_proj = nn.Linear(\n            self.hidden_size, self.intermediate_size, bias=config.mlp_bias\n        )\n        self.down_proj = nn.Linear(\n            self.intermediate_size, self.hidden_size, bias=config.mlp_bias\n        )\n        if config.hidden_act != \"silu\":\n            raise NotImplementedError(\n                \"Only Libra SiLU has been implemented for this model currently.\"\n            )\n\n        self.act_fn = libra_silu\n\n    def forward(\n        self,\n        x,\n        libra_p=\"from_global\",\n    ):\n        if libra_p == \"from_global\":\n            libra_p = global_libra_mode_p\n\n        # ic(self.act_fn)\n        \n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [\n                    F.linear(x, gate_proj_slices[i])\n                    for i in range(self.config.pretraining_tp)\n                ],\n                dim=-1,\n            )\n            up_proj = torch.cat(\n                [\n                    F.linear(x, up_proj_slices[i])\n                    for i in range(self.config.pretraining_tp)\n                ],\n                dim=-1,\n            )\n\n            fused_stream = self.act_fn(gate_proj) * up_proj\n            if libra_p:\n                print_libra(\"Libra Self-Gating\")\n                fused_stream = swap_backward(fused_stream, fused_stream / 2)\n\n            intermediate_states = (fused_stream).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i])\n                for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n            \n        else:\n            fused_stream = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n            if libra_p:\n                print_libra(\"Libra Self-Gating\")\n                fused_stream = swap_backward(fused_stream, fused_stream / 2)\n\n            down_proj = self.down_proj(fused_stream)\n\n        return down_proj"]},{"cell_type":"markdown","id":"229d8192-b1f4-4664-b6a2-4acab99cb10e","metadata":{},"source":["#### LayerNorm\n\n"]},{"cell_type":"code","execution_count":1,"id":"cf100414-a12c-4f68-8c06-be771ac40912","metadata":{},"outputs":[],"source":["class LibraLlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states, libra_p=\"from_global\"):\n        if libra_p == \"from_global\":\n            print_libra(\"Libra LayerNorm\")\n            \n            libra_p = global_libra_mode_p\n\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        denom = torch.rsqrt(variance + self.variance_epsilon)\n        if libra_p:\n            denom = denom.detach()\n        \n        hidden_states = hidden_states * denom\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""]},{"cell_type":"markdown","id":"6f2af853-03e4-4fa4-b1b4-1ae464a2151d","metadata":{},"source":["#### Attention\n\n"]},{"cell_type":"markdown","id":"4de7a4ff-b6b0-4628-92ed-0d1619bbb0a2","metadata":{},"source":["##### Patch `F.scaled_dot_product_attention`:\n\n"]},{"cell_type":"code","execution_count":1,"id":"1001bcf9-0501-4559-a9cf-ca290d7bc0da","metadata":{},"outputs":[],"source":["#: If `scaled_dot_product_attention_orig` is not already defined (useful for when this code is reloaded):\nif not \"scaled_dot_product_attention_orig\" in globals():\n    scaled_dot_product_attention_orig = F.scaled_dot_product_attention\n\n\ndef scaled_dot_product_attention_patched(\n    query,\n    key,\n    value,\n    attn_mask=None,\n    dropout_p=0.0,\n    is_causal=False,\n    scale=None,\n    libra_p=\"from_global\",\n    **kwargs,\n):\n    if libra_p == \"from_global\":\n        libra_p = global_libra_mode_p\n\n    if libra_p:\n        print_libra(\"Libra Attention\")\n        \n        query = query.detach()\n        key = key.detach()\n\n    return scaled_dot_product_attention_orig(\n        query=query,\n        key=key,\n        value=value,\n        attn_mask=attn_mask,\n        dropout_p=dropout_p,\n        is_causal=is_causal,\n        scale=scale,\n        **kwargs,\n    )\n\n\nF.scaled_dot_product_attention = scaled_dot_product_attention_patched"]},{"cell_type":"markdown","id":"d5c462ab-1574-482a-96f9-7f505ca9d6a9","metadata":{},"source":["#### Replace Modules With Their Libra Counterparts\n\n"]},{"cell_type":"markdown","id":"d6de50e1-84e5-4ea8-8823-bc54bce60ffc","metadata":{},"source":["We&rsquo;re going to swap out parts of our PyTorch model using a helper function called `module_mapper` (imported from `PyNight` earlier). Just keep in mind that this function is more of a hack rather than an officially supported solution, so we should double-check that everything works correctly after making these changes.\n\n"]},{"cell_type":"code","execution_count":1,"id":"7387586e-ab4a-4f7e-9b92-90371d2bee26","metadata":{},"outputs":[],"source":["from transformers.models.llama.modeling_llama import (\n    LlamaRMSNorm,\n    LlamaMLP,\n)"]},{"cell_type":"markdown","id":"10b310bc-49e0-4dfe-9086-6e349a23a855","metadata":{},"source":["We need all constructor arguments to be present as attributes to be able to replace modules using `module_mapper`. So we need to store some attributes manually:\n\n"]},{"cell_type":"code","execution_count":1,"id":"a5de432e-710c-4265-a73e-c712868c048a","metadata":{},"outputs":[],"source":["def store_hidden_size_for_LlamaRMSNorm(model):\n    for module in model.modules():\n        if isinstance(module, LlamaRMSNorm):\n            #: The weight shape is (hidden_size,), so we take the first dimension\n            module.hidden_size = module.weight.shape[0]\n\n    return model\n\nmodel = store_hidden_size_for_LlamaRMSNorm(model)"]},{"cell_type":"code","execution_count":1,"id":"361d9436-a108-40df-85bc-0deed50c2b67","metadata":{},"outputs":[],"source":["module_mapping = {\n    LlamaRMSNorm: LibraLlamaRMSNorm,\n    LlamaMLP: LibraLlamaMLP,\n    nn.SiLU: LibraSiLU,\n    #: We have globally patched `F.scaled_dot_product_attention`, so no need for further modifications.\n}\n\nlibra_model = module_mapper(model, module_mapping).new_model\nlibra_model.to(device)\n\nlibra_model"]},{"cell_type":"markdown","id":"f588e6a1-4dd4-43b8-a902-649db20f498d","metadata":{},"source":["Now we will verify that the forward pass has not changed after our modifications:\n\n"]},{"cell_type":"code","execution_count":1,"id":"5a0170f3-c3f1-4de8-a3a2-0592a206d373","metadata":{},"outputs":[],"source":["libra_pred = predict_next_token(text, model=libra_model)\n\n#: Verify predictions match\nassert original_pred.next_token_id == libra_pred.next_token_id\n\n#: Compare logits\ncomparison = compare_logits(original_pred.logits.squeeze().cpu(), libra_pred.logits.squeeze().cpu())\nprint_logits_comparison(comparison)"]},{"cell_type":"markdown","id":"ec5bbfb0-d6ba-4688-92fb-0341b812eac0","metadata":{},"source":["#### Attribution Tests on Libra\n\n"]},{"cell_type":"code","execution_count":1,"id":"bb292db4-d202-445b-8a87-5cca8309d92e","metadata":{},"outputs":[],"source":["analyze_test_cases(\n    test_cases=[\n        \"You can type whatever you want here. E.g., the capital of France is\"\n    ],\n    model=model,\n    plot_ixg=True,\n    plot_fullgrad=True\n)"]},{"cell_type":"code","execution_count":1,"id":"32a37f44-9962-420c-ba75-68ddf461a4b6","metadata":{},"outputs":[],"source":["analyze_test_cases(\n    test_cases=all_test_cases,\n    model=model,\n    plot_ixg=True,\n    plot_fullgrad=True\n)\n\nprint(\"\\n\\nFinished.\")"]}],"metadata":[["org"],null,null],"nbformat":4,"nbformat_minor":5}
